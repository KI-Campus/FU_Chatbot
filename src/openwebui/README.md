# OpenWebUI Arena Integration f√ºr KI-Campus Chatbot

Dieses Verzeichnis enth√§lt die OpenWebUI-Integration f√ºr das Chatbot Arena Benchmarking **mit Voting System & Statistik-Logging**.

## üéØ Status

‚úÖ **Arena Mode funktioniert!** - Side-by-Side Vergleiche laufen  
‚úÖ **Voting System Live!** - Web Dashboard + CLI Tool zum Abstimmen  
‚úÖ **Vollst√§ndiges Logging** - Alle Vergleiche und Votes persistent gespeichert  
‚úÖ **Bereit f√ºr Massentests** - Starte mit `./start_arena.sh`

## üåü Highlights

- **OpenWebUI Arena Mode** - Zwei Modelle parallel im Chat vergleichen
- **Voting Dashboard** (Port 8002) - Sch√∂nes Web-Interface zum Voten
- **Automatisches Logging** - Alle Votes in JSONL Format gespeichert
- **Live Statistiken** - Win Rates, Unentschieden, Trends
- **CLI Tools** - Voting via Terminal, Batch Processing, Exports
- **Azure OpenAI Integration** - Echte GPT-4 Antworten mit Citations

## üìä Architektur

```
OpenWebUI (3001)      Voting UI (8002)
    ‚Üì                      ‚Üì
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚Üì
         LLM-API (8001)
        /      |      \
    Chat   Voting   Storage
  (Azure)  (API)    (JSONL)
```

## üöÄ Quick Start

```bash
# Alle Services starten (einmalig nach Neustart)
./start_arena.sh
```

Dann √∂ffne:
- **Chat**: http://localhost:3001
- **Voting**: http://localhost:8002

## Dateien

- `openwebui_api_simple.py`: **[AKTIV]** Mock-API mit Streaming f√ºr Arena Mode Tests
- `openwebui_api_llm.py`: Vollst√§ndige API mit Azure OpenAI Integration (ben√∂tigt Azure Auth)
- `openwebui_api.py`: Original API (deprecated, hat Langfuse-Probleme)
- `assistant_improved.py`: Verbesserte Version des KI-Campus Assistenten
- `arena_benchmark.py`: CLI-Tool f√ºr manuelle Benchmarks (Alternative zu Arena Mode)
- `Dockerfile`: Docker-Image f√ºr den OpenWebUI API-Service
- `requirements.txt`: Python-Dependencies f√ºr den Service

## Setup

### 1. Voraussetzungen

- Docker und Docker Compose installiert
- OpenWebUI l√§uft (siehe [OpenWebUI Docs](https://docs.openwebui.com/))
- Zugriff auf die KI-Campus Vector-Datenbank (Qdrant)

### 2. Service starten

#### Option A: Mit Docker Compose (empfohlen)

```bash
# Im Root-Verzeichnis des Projekts
docker-compose up openwebui-api
```

#### Option B: Standalone Docker

```bash
# Docker Image bauen
docker build -t kicampus-openwebui -f src/openwebui/Dockerfile .

# Container starten
docker run -p 8001:8001 \
  --env-file .env \
  kicampus-openwebui
```

#### Option C: Lokale Entwicklung (Aktuell empfohlen f√ºr Tests)

```bash
# Dependencies installieren (falls noch nicht geschehen)
pip install fastapi uvicorn pydantic

# Mock-API mit Streaming-Support starten
python -m uvicorn src.openwebui.openwebui_api_simple:app --host 0.0.0.0 --port 8001

# F√ºr echte LLM-Antworten (ben√∂tigt Azure Login):
# python -m uvicorn src.openwebui.openwebui_api_llm:app --host 0.0.0.0 --port 8001
```

### 3. OpenWebUI konfigurieren

1. OpenWebUI sollte bereits laufen auf `http://localhost:3001`
2. Gehe zu **Settings** ‚Üí **Connections** ‚Üí **OpenAI API**
3. F√ºge eine neue Connection hinzu:
   - **Base URL**: `http://host.docker.internal:8001/v1`
   - **API Key**: (optional, kann leer bleiben)
   - **Verify** klicken - sollte beide Modelle finden

4. **Arena Mode aktivieren**:
   - √ñffne einen neuen Chat
   - Klicke oben auf das Modell-Dropdown
   - W√§hle **"Arena (Side-by-side)"**
   - W√§hle beide Modelle aus: `kicampus-original` und `kicampus-improved`

### 4. Arena-Benchmarking durchf√ºhren - **FUNKTIONIERT JETZT! ‚úÖ**

1. √ñffne einen neuen Chat in OpenWebUI (`http://localhost:3001`)
2. Klicke oben auf das Modell-Dropdown und w√§hle **"Arena (Side-by-side)"**
3. W√§hle beide Modelle: `kicampus-original` und `kicampus-improved`
4. Stelle Fragen - beide Modelle antworten parallel mit Streaming
5. Bewerte die Antworten und vergleiche die Qualit√§t

**Hinweis**: Die aktuelle Mock-API gibt Demo-Antworten mit Pr√§fix `[Original]` bzw. `[Verbessert]` zur√ºck.  
F√ºr echte LLM-Antworten muss `openwebui_api_llm.py` verwendet werden (ben√∂tigt stabile Azure Auth).

## API-Endpoints

Der Service bietet folgende OpenWebUI-kompatible Endpoints:

### `GET /`
Health check und Service-Info

### `GET /v1/models`
Liste aller verf√ºgbaren Modelle

### `POST /v1/chat/completions`
Chat Completion Endpoint (OpenAI-kompatibel)

**Request:**
```json
{
  "model": "kicampus-original",
  "messages": [
    {"role": "user", "content": "Was ist Deep Learning?"}
  ],
  "stream": false
}
```

**Response:**
```json
{
  "id": "chatcmpl-123",
  "object": "chat.completion",
  "created": 1700000000,
  "model": "kicampus-original",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Deep Learning ist..."
      },
      "finish_reason": "stop"
    }
  ]
}
```

## Verbesserungen implementieren

Die verbesserte Version (`kicampus-improved`) befindet sich in `assistant_improved.py`. 

### Aktuelle Verbesserungen:
- Erh√∂htes Chat-History-Limit (10 ‚Üí 15 Nachrichten)
- Erweiterter Kontext f√ºr bessere Antworten

### Weitere Optimierungen hinzuf√ºgen:

1. **Bessere Retrieval-Strategie:**
```python
def retrieve_with_reranking(self, query: str):
    # Multi-stage retrieval mit Re-Ranking
    initial_chunks = self.retriever.retrieve(query, top_k=20)
    reranked_chunks = self.reranker.rerank(query, initial_chunks, top_k=5)
    return reranked_chunks
```

2. **Verbesserte Prompts:**
```python
IMPROVED_SYSTEM_PROMPT = """
Du bist ein hilfreicher KI-Assistent f√ºr KI-Campus.
Antworte pr√§zise und strukturiert...
"""
```

3. **Query-Expansion:**
```python
def expand_query(self, query: str) -> str:
    # Erweitere die Query f√ºr besseres Retrieval
    expanded = self.llm.generate(
        f"Generiere 2-3 alternative Formulierungen f√ºr: {query}"
    )
    return f"{query} {expanded}"
```

## Benchmarking-Metriken

Folgende Aspekte sollten beim Benchmarking beachtet werden:

- **Antwortqualit√§t**: Pr√§zision und Vollst√§ndigkeit der Antworten
- **Quellenverwendung**: Korrekte Zitation von Kursmaterialien
- **Sprachqualit√§t**: Nat√ºrlichkeit und Verst√§ndlichkeit
- **Relevanz**: Passung der Antwort zur Frage
- **Kontextverst√§ndnis**: Ber√ºcksichtigung des Chat-Verlaufs

## Troubleshooting

### Service startet nicht
```bash
# Logs pr√ºfen
docker logs <container-id>

# Port-Konflikt?
lsof -i :8001
```

### OpenWebUI findet die Modelle nicht
- Pr√ºfe, ob der Service l√§uft: `curl http://localhost:8001/v1/models`
- √úberpr√ºfe die Base URL in OpenWebUI
- Stelle sicher, dass keine CORS-Probleme vorliegen

### Antworten sind identisch
- Beide Versionen nutzen aktuell die gleiche Basis-Implementierung
- Implementiere Verbesserungen in `assistant_improved.py`

## Weiterf√ºhrende Ressourcen

- [OpenWebUI Documentation](https://docs.openwebui.com/)
- [OpenAI API Specification](https://platform.openai.com/docs/api-reference)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)

## Lizenz

Siehe root LICENSE file.
