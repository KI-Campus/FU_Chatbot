================================================================================
DOCUMENT TEXT
================================================================================

Module Name: Welcome

Text: 



Welcome


Welcome to Part 1 of the Foundations of Deep Learning Specialization!
 
        


In this section, we’ll begin our journey by laying the groundwork for understanding deep learning. We’ll start with an introduction to deep learning, followed by basic concepts such as multilayer perceptrons (MLPs), and backpropagation. 




We’ll also cover key optimization techniques and important regularization methods to help you build robust neural networks. By the end of this course, you'll have a solid understanding of the core concepts that will support your learning in more advanced topics.




Let’s dive right in!








What You Will Learn






Introduction to Deep Learning
: We will start with an overview of why deep learning is the key technology of our time, how it differs from classical machine learning, and why it is so powerful in solving complex problems.


From Logistic Regression to MLPs
: You will learn how logistic regression evolves into multi-layer perceptions (MLPs). You will also learn about activation and loss functions for deep learning. 


MLPs and Backpropagation:
 We will take a closer look at the mechanics of the backpropagation algorithm, which enables neural networks to learn from data.


Optimization of Neural Networks: 
We will discuss various optimization algorithms, starting with the basics of gradient-based Optimization, and delvng deeper into stochastic gradient descent (SGD) and adaptive gradient algorithms.


Regularization: 
We will discuss various regularization methods such as L1 and L2 regularization, dropout, and data augmentation, which improve the generalization capacity of eep learning models.




By the end of this part, you will have a solid foundation in the basic principles of deep learning, and be ready to tackle more advanced topics in the subsequent parts.






Course Structure


Videos


Each chapter contains several video lectures with interactive questions. In addition, the video lectures include a final slide with additional questions for independent study or discussion with others. We strongly recommend that you complete both of these activities to activate each week's material.


Quizzes


At the end of each chapter there is an additional self-test quiz to help you assess your own understanding of the topic. At the very end of the course, there is also a final graded assessment with combined questions from all the topics covered in the course.


Exercises


Each chapter also contains its own coding exercises in Python. Each exercise includes detailed unit tests that you can use to assess whether your solution matches our master solution. We kindly ask you not to distribute your own solutions, so as not to spoil the course for others.










Our Team


Prof. Frank Hutter
: Frank Hutter is a Hector-Endowed Fellow and PI at the ELLIS Institute Tübingen, as well as Full Professor for Machine Learning at the University of Freiburg (Germany). Frank holds a PhD from the University of British Columbia (UBC, 2009) and a Diplom (eq. MSc) from TU Darmstadt (2004). 


Frank is best known for his research on automated machine learning (AutoML), including neural architecture search, efficient hyperparameter optimization, and meta-learning. He co-authored the first 
book on AutoML
 and the prominent AutoML tools Auto-WEKA, Auto-sklearn and Auto-PyTorch, won the first two AutoML challenges with his team, is co-teaching the first 
MOOC on AutoML
, co-organized 15 AutoML-related workshops at ICML, NeurIPS and ICLR, and founded the AutoML conference as general chair in 2022 and 2023.


In recent years, his focus has been on the intersection of foundation models and AutoML, including the first foundation model for tabular data, TabPFN, and improving pretraining and fine-tuning with AutoML.




Prof. Abhinav Valada
: Abhinav Valada is a Full Professor (W3) at the University of Freiburg, where he directs the Robot Learning Lab. He is a member of the Department of Computer Science, the BrainLinks-BrainTools center, and a founding faculty of the ELLIS unit Freiburg.


He received his PhD working with Prof. Wolfram Burgard at the University of Freiburg in 2019, his MS in Robotics from Carnegie Mellon University in 2013, and his BTech. in Electronics and Instrumentation Engineering from VIT University in 2010. 


Abhinav’s research lies at the intersection of robotics, machine learning, and computer vision with a focus on tackling fundamental robot perception, state estimation, and planning problems to enable robots to operate reliably in complex and diverse domains. 
The overall goal of his research is to develop scalable lifelong robot learning systems that 
continuously learn
 multiple tasks from what they 
perceive
 and 
experience
 by interacting with the real world.












Course Links



            Here are the links to the other two courses:
            


Foundations of Deep Learning II: Architectures & Methodology


Foundations of Deep Learning III: Advanced Topics





Transcript:
None
None
None
None

================================================================================
METADATA
================================================================================

{
  "course_id": 235,
  "module_id": 19980,
  "fullname": "Welcome",
  "type": "module",
  "url": "https://moodle.ki-campus.org/mod/page/view.php?id=19980"
}