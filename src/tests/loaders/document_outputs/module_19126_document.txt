================================================================================
DOCUMENT TEXT
================================================================================

Module Name: Waterpark as MDP

Beschreibung: In this grid-waterpark we will look at discounting, finite horizons and value iteration in more detail.

--- Course Presentation ---
[Seite 1]:
[Text] Waterpark as MDP
[Seite 2]:
[Text] Waterpark as MDP
[Text] Consider the shown MDP. The state space consists of all squares in a grid-world water park. There is a single waterslide that is composed of two ladder squares and two slide squares (marked with vertical bars and squiggly lines respectively). An agent in this water park can move from any square to any neighbouring square, unless the current square is a slide in which case it must move forward one square along the slide. The actions are denoted by arrows between squares on the map and all deterministically move the agent in the given direction. The agent cannot stand still: it must move on each time step. Rewards are also shown: the agent feels great pleasure as it slides down the water slide (+2), a certain amount of discomfort as it climbs the rungs of the ladder (-1), and receives rewards of 0 otherwise. The time horizon is infinite; this MDP goes on forever.
[Seite 3]:
[Text] Waterpark as MDP
[Lückentext] How many (deterministic) policies \(\pi\) are possible for this MDP?
*2048*
[Seite 4]:
[Text] Waterpark as MDP
[Text] The waterslide MDP has 14 grid-states in total. In 3 out of the 14 states the agent has only one allowed action, so any policy has one fixed value for these states. In the other 11 states, the agent has two choices. Each possible combinations of actions over these 11 states is a valid policy. So we have 2 options for square A, times 2 for square B times 2 ... times 2 .... in total \(2^{11}\) possible policies.
[Seite 5]:
[Text] Waterpark as MDP
[Lückentext] What is the optimal value of state A and E if the game ends in 1 step without discounting? So what is \(V^{\star}_1(A)\) and \(V^{\star}_1(E)\) with \(\gamma = 1\)?
\(V^{\star}_1(A)\) = *0* \(V^{\star}_1(E)\) = *2*
[Seite 6]:
[Text] \(V^*_1(s) = \max_a \sum_{s'} T(s, a, s') [R(s, a, s') + \gamma V^*_0(s')]\) The actions are deterministic, so we have for each action only one outcome and don't have to sum over the different transitions. \(\gamma=1\) and \(V^{\star}_0 = 0\) for all states (we don't have any timesteps left, so we cannot do anything, so we cannot get any future rewards). From A, we can choose to either go up or go right. \(V^*_1(A) = \max_{up, right} ([R(A, up, B) + \gamma V^*_0(B)], [R(A, right, N) + \gamma V^*_0(N)]\) (\(T(A, up, B) = T(A, right, N) = 1\) all others are 0) If we go up, we receive a reward of 0. If we go right, we also receive a reward of 0. \(V^*_1(A) = \max_{up, right} (0, 0) = 0\) From E, we can choose to either go left or go right. \(V^*_1(E) = \max_{left, right} ([R(E, left, D) + \gamma V^*_0(D)], [R(E, right, F) + \gamma V^*_0(F)]\) (\(T(E, left, D) = T(E, right, F) = 1\) all others are 0) If we go left, we receive a reward of 0. If we go right, we receive a reward of 2. \(V^*_1(E) = \max_{left, right} (0, 2) = 2\)
[Seite 7]:
[Text] Waterpark as MDP
[Lückentext] What is the optimal value of state A and E if the game ends in 3 steps without discounting? So what is \(V^{\star}_3(A)\) and \(V^{\star}_3(E)\) with \(\gamma = 1\)?
\(V^{\star}_3(A)\) = *0* \(V^{\star}_3(E)\) = *4*
[Seite 8]:
[Text] Within three timesteps we cannot reach any rewards from A, so the value of A stays 0. From E, the best we can do is slide down the waterslide, which gives a reward of 2 from E to F and a reward of 2 from F to G. In the third time step, we cannot gain any rewards, so in total we have 4. More formally we could do that with 3 steps of value iteration. In step 0 we start with a value of 0 for all states. In step 1, we do what we did in the previous question: \(V_1^*(s) = \max_a (R(s,a,s') + V_0^*(s'))\). The transition functions can be ignored as our actions are deterministic. In step 2 we do: \(V_2^*(s) = \max_a (R(s,a,s') + V_1^*(s'))\). This is very similar to the step before, only we use here now \(V_1^*(s')\) and not \(V_0^*(s')\). Step 3 is again very similar: \(V_3^*(s) = \max_a (R(s,a,s') + V_2^*(s'))\). We now only use \(V_2^*(s')\). The table below shows \(R(s,a,s') + V_{step-1}^*(s')\) and the maximum of both for different steps and states.
[Seite 9]:
[Text] Waterpark as MDP
[Lückentext] What is the optimal value of state A and E if the game ends in 3 steps with discounting \(\gamma = 0.1\)?
\(V^{\star}_3(A)\) = *0* \(V^{\star}_3(E)\) = *2.2*
[Seite 10]:
[Text] As previously, we cannot reach any rewards from A within three timesteps, so the value of A stays 0. From E, the best we can do is slide down the waterslide, which gives a reward of 2 from E to F and a reward of 2 from F to G. In the third time step, we cannot gain any rewards. Now we need to take the discount into consideration: from E to F we get 2 (reward of the very next action, so no discount) but then from F to G we would get a reward from 2, but we need to discount it with a factor of 0.1. The 0 rewards from G to H we would need to discount with a factor of 0.01. So in total we have \(2 + 0.1\cdot2 + 0.01\cdot0 = 2.2\). We can solve this again formally with value iteration. The steps are the same as without discount, we only need to take the discount into account: \(V_{k+1}^*(s) = \max_a (R(s,a,s') +\gamma V_{k}^*(s'))\). The table below shows \(R(s,a,s') + \gamma V_{step-1}^*(s')\) and the maximum of both for different steps and states.
[Seite 11]:
[Text] If you compare the solution tables for \(V^*_3\) with and without discount, you will only see a few changes. The most important change is that a few values are now 0, which were positive before. As we first have to climb the ladder, before we can slide down (except if we start in E or F), the negative rewards get discounted less than the positive rewards from sliding down the slide. For example consider starting in D: D to E gives a reward of -1, E to F a reward of 2 wich needs to be discounted by \(\gamma = 0.1\), F to G gives again a reward of +2 but now discounted by \(\gamma^2=0.01\) so in total \(-1 + \gamma 2 + \gamma^2 2 = -0.78\). This becomes even worse if we start in C: C-D gives -1, D-E gives -1, E-F gives 2 and F-G gives 2, however we multiply each reward by a factor of gamma more: \(-1 + \gamma( -1) + \gamma^2 2 + \gamma^3 2 = -1.078\). If you start even earlier, these numbers will be multiplied by a few factors of \(\gamma\), but will stay negative. So the better option is to not go to the slide, but run around where we get 0 rewards.
[Seite 12]:
[Text] Waterpark as MDP
[Lückentext] Let's jump ahead. What is the optimal value of state A and E if the game ends in 10 steps with and without discounting?
\(\gamma = 1\): \(V^{\star}_{10}(A)\) = *2* \(V^{\star}_{10}(E)\) = *4*
[Seite 13]:
[Text] From A, we can now reach the waterslide. We get -1 from C to D and -1 from D to E for climbing up the ladder. Then we get +2 from E to F and +2 from F to G for sliding down. Afterwards, we can only receive 0 rewards, so we have in total 2. Without discount, we have in total 2. However with discount, sliding down the waterslide doesn't make up for climbing up the ladder (see before), so we settle for getting 0 rewards, so we have in total 0. For E, we will slide done the waterslide and gain rewards of +2 from E to F and +2 from F to G. Afterwards we have 8 steps left, but cannot reach any more rewards. So without discount we have 4 in total. With discount the reward we get from F to G is discounted by \(\gamma=0.1\), so we have in total \(2 +0.1\cdot 2 = 2.2\). We can do that again with value iteration. Look at the previous slides for an example. All you would need to to is fill the table with 7 further rows, but they work exactly the same as the rows of step 1 to 3.
[Seite 14]:
[Text] Waterpark as MDP
[Lückentext] Let's do a short detour and look at Q-Values. What are the following Q-Values? Assume that \(\gamma=1\).
\(Q^{\star}_{1}(E, left)\) = *0* \(Q^{\star}_{10}(E, left)\) = *3*
[Seite 15]:
[Text] For Q-Values the calculation is only slightly different to the calculation of V-values: \(Q^*(s, a) = \sum_{s'} T(s, a, s') [R(s, a, s') + \gamma V^*(s')]\) In this step we are bound to action a, but afterwards we act optimally. So for \(Q_1^*(E, left)\) we have to go left. That gives no reward and \(V_0^*(D) = 0\) (there are no more steps to go, so no rewards to get). For \(Q_{10}^*(E, left)\) we also have to go left. We land in D and have collected 0 rewards. Now we are free to choose. As we don't have a discount, the best option is to go back right, receive a reward of -1 and then slide down the water slide, collecting 2 times a reward of 2. Afterwards we can only receive 0 rewards within 6 time steps, so we get in total 3. We can also calculate this values with (Q-)Value iteration. Look into the solution tables given for the 3 steps. \(V^*(s) = \max_a Q^*(s, a)\). The values labeled with up, down, right or left are the Q-Values for that state and the given action. To get the value we take the maximum of the Q-Values.
[Seite 16]:
[Text] Waterpark as MDP
[Lückentext] Now assume the MDP never ends. What is the optimal value of state A and E with discounting?
\(\gamma = 0.1\): \(V^{\star}(A)\) = *0* \(V^{\star}(E)\) = *2.2*
[Seite 17]:
[Text] We follow here the same argumentation as for 10 steps. If we climb up the ladder and slide down the waterslide, the discount makes the negative rewards count more than the positives, so in total we would get negative rewards from that. So for starting in A, it is better to wander around and not receive any rewards, than head for the waterslide. From E, we can slide down the waterslide, as we don't have to climb the ladder. For that we get rewards of 2 + 0.1*2 = 2.2. But we will not go for the slide again as we are now in a similar position as we were for A. So from now on, we only collect rewards of 0. This would look very different if we didn't have a discount. Then we could go for the waterslide again and again, collecting positive rewards. So for both A and B \(V^*\) would be infinity.

================================================================================
METADATA
================================================================================

{
  "course_id": 41,
  "module_id": 19126,
  "fullname": "Waterpark as MDP",
  "type": "module",
  "url": "https://moodle.ki-campus.org/mod/h5pactivity/view.php?id=19126"
}